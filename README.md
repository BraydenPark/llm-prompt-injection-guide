# llm-prompt-injection-guide
Develop a clear, structured, and repeatable red teaming guide that demonstrates how to test large language models (LLMs) for prompt injection vulnerabilities.
